[2024-10-16 10:47:59,143][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
processing:
  batch_size: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2024-10-16 10:47:59,144][__main__][INFO] - Using the model: google/bert_uncased_L-2_H-128_A-2
[2024-10-16 10:47:59,144][__main__][INFO] - Using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2024-10-16 10:48:22,823][datasets.load][WARNING] - Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
[2024-10-16 10:48:22,845][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'cola' at C:\Users\saura\.cache\huggingface\datasets\glue\cola\0.0.0\bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Wed Oct  2 08:46:48 2024).
